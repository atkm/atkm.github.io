<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Atsuya Kumano</title>
    <link>http://akumano.site/posts/</link>
    <description>Recent content in Posts on Atsuya Kumano</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 06 Dec 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="http://akumano.site/posts/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Learning Resources - Computer Science (Systems)</title>
      <link>http://akumano.site/posts/learning-resources-cs/</link>
      <pubDate>Thu, 06 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>http://akumano.site/posts/learning-resources-cs/</guid>
      <description>Freely available online resources that I&amp;rsquo;ve found useful for learning computer science, especially systems. Last updated on 2019-11-23.
Systems Beyond CMU 15-213.
 CMU 15-440 (Fall version)
 The textbook (Van Steen &amp;amp; Tanenbaum) is readable. The slides are informative as supplements to corresponding textbook chapters and referenced articles. Lab assignments are done in Go. Tons of debugging concurrency issues.  Go warm-up. A simple key-value server. Implement a subset of TCP starting with UDP.</description>
    </item>
    
    <item>
      <title>Notes: Curse of Dimensionality</title>
      <link>http://akumano.site/posts/curse-of-dimensionality-notes/</link>
      <pubDate>Sat, 20 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>http://akumano.site/posts/curse-of-dimensionality-notes/</guid>
      <description>&amp;ldquo;Curse of Dimensionality&amp;rdquo;, in the sense of Bellman as well as in machine learning.
Geometry In \( \mathbb{R}^n \) where \( n \) is large,
 The volume of the unit ball goes to 0 as the dimension goes to infinity. Most of the volume of a n-ball is concentrated around its boundary. Pick two vectors on the surface of the unit ball independently. The two are orthogonal with high probability.</description>
    </item>
    
    <item>
      <title>Articles---mainly about Streaming Systems</title>
      <link>http://akumano.site/posts/taxi-project-reading-list/</link>
      <pubDate>Tue, 21 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>http://akumano.site/posts/taxi-project-reading-list/</guid>
      <description>Articles and talks that I referred to while working on the taxi data project. Published here as a note to myself.
Model Serving  FLIP-23. The document also discusses implementing model training as well as model serving. Two linked documents&amp;mdash;&amp;ldquo;Flink ML Roadmap&amp;rdquo; and &amp;ldquo;Flink-MS&amp;rdquo;&amp;mdash;are also worth reading. Boris Lublinsky&amp;rsquo;s book &amp;ldquo;Serving Machine Learning Models&amp;rdquo;, and talk.  Distributed Systems  Please Stop Calling Database Systems AP or CP. Kate Matsudaira on distributed systems Distributed Systems for Fun and Profit Martin Kleppmann&amp;rsquo;s book and interview.</description>
    </item>
    
    <item>
      <title>Learning Resources</title>
      <link>http://akumano.site/posts/learning-resources/</link>
      <pubDate>Wed, 04 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>http://akumano.site/posts/learning-resources/</guid>
      <description>Freely available online resources that I&amp;rsquo;ve found useful for learning Statistics, Machine Learning, Distributed Computing, Database Systems, and other CS and SWE topics.
Statistics and Machine Learning ML Expositions on machine learning that are freely accessible (like the ones on Coursera) tend to sweep theoretical foundation and mathematical rigor under the carpet; these are resources that don&amp;rsquo;t skimp on the hard stuff.
 Stanford CS229 Machine Learning
 One-line summary: the course teaches you how to set up a cost function based on a model and data, and figure out how to optimize it.</description>
    </item>
    
    <item>
      <title>Simple Linear Regression with Heteroskedastic Noise</title>
      <link>http://akumano.site/posts/heteroskedasticity/</link>
      <pubDate>Fri, 04 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>http://akumano.site/posts/heteroskedasticity/</guid>
      <description>Introduction The model we consider is \(Y_i = \alpha + \beta x_i + \epsilon_i\), where \( \epsilon_i \) are uncorrelated, and \( \mathbb{V}(\epsilon_i) \) depends on \( i \). We discuss two solutions to finding estimators of \( \alpha, \beta \). Weighted least squares regression leads to best linear unbiased estimators (BLUE). Also, with stronger assumptions on \( \epsilon_i \), maximum likelihood estimators (MLE) can be found. We begin with a discussion of the homoskedastic case with an emphasis on relations between statistical properties of the least squares estimators and assumptions on \( \epsilon_i \), which is conducive to understanding the heteroskedastic case.</description>
    </item>
    
  </channel>
</rss>